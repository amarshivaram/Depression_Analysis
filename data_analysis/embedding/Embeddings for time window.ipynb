{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset = pd.read_csv(\"D:\\Workspace\\OVGU\\SM_Depression\\src\\depTweetsOrderedByDateTimeWindowURLTokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61972 entries, 0 to 61971\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   accountname       61972 non-null  object \n",
      " 1   description       61483 non-null  object \n",
      " 2   favorite_count    61937 non-null  float64\n",
      " 3   location          51551 non-null  object \n",
      " 4   masked_user_id    61972 non-null  object \n",
      " 5   retweet_count     61968 non-null  object \n",
      " 6   timestamp         61972 non-null  object \n",
      " 7   tweets            61972 non-null  object \n",
      " 8   twittername       61972 non-null  object \n",
      " 9   user_type         61972 non-null  float64\n",
      " 10  format            61972 non-null  int64  \n",
      " 11  new_date          61972 non-null  object \n",
      " 12  time_window       61972 non-null  int64  \n",
      " 13  tweets_url_token  61972 non-null  object \n",
      " 14  cleanData         61728 non-null  object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "depressed_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountname</th>\n",
       "      <th>description</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>location</th>\n",
       "      <th>masked_user_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweets</th>\n",
       "      <th>twittername</th>\n",
       "      <th>user_type</th>\n",
       "      <th>format</th>\n",
       "      <th>new_date</th>\n",
       "      <th>time_window</th>\n",
       "      <th>tweets_url_token</th>\n",
       "      <th>cleanData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:00</td>\n",
       "      <td>@Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>@Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...</td>\n",
       "      <td>it's about time. Lol IT'S NOT FAIR [URL] [URL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:02</td>\n",
       "      <td>@w_terrence @PearlCathey8 Like deplorable, lik...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:02</td>\n",
       "      <td>1</td>\n",
       "      <td>@w_terrence @PearlCathey8 Like deplorable, lik...</td>\n",
       "      <td>Like deplorable, like donnie, have to use cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:05</td>\n",
       "      <td>@HarleyQuinnlif3 Dick always ruin things lol</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:05</td>\n",
       "      <td>1</td>\n",
       "      <td>@HarleyQuinnlif3 Dick always ruin things lol</td>\n",
       "      <td>Dick always ruin things lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:13</td>\n",
       "      <td>After seeing pics of todays rally in Phoenix I...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing pics of todays rally in Phoenix I...</td>\n",
       "      <td>After seeing pics of todays rally in Phoenix I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:13</td>\n",
       "      <td>if I do get covid19 becuase I'm surrounded by ...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>if I do get covid19 becuase I'm surrounded by ...</td>\n",
       "      <td>if I do get covid19 becuase I'm surrounded by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accountname  \\\n",
       "0  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "1  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "2  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "3  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "4  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "\n",
       "                                         description  favorite_count  \\\n",
       "0  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             0.0   \n",
       "1  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             0.0   \n",
       "2  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             0.0   \n",
       "3  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             2.0   \n",
       "4  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             2.0   \n",
       "\n",
       "                  location     masked_user_id retweet_count         timestamp  \\\n",
       "0  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:00   \n",
       "1  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:02   \n",
       "2  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:05   \n",
       "3  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:13   \n",
       "4  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:13   \n",
       "\n",
       "                                              tweets     twittername  \\\n",
       "0  @Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...  CmdrTinkerBell   \n",
       "1  @w_terrence @PearlCathey8 Like deplorable, lik...  CmdrTinkerBell   \n",
       "2       @HarleyQuinnlif3 Dick always ruin things lol  CmdrTinkerBell   \n",
       "3  After seeing pics of todays rally in Phoenix I...  CmdrTinkerBell   \n",
       "4  if I do get covid19 becuase I'm surrounded by ...  CmdrTinkerBell   \n",
       "\n",
       "   user_type  format          new_date  time_window  \\\n",
       "0        1.0       1  2020-06-24 00:00            1   \n",
       "1        1.0       1  2020-06-24 00:02            1   \n",
       "2        1.0       1  2020-06-24 00:05            1   \n",
       "3        1.0       1  2020-06-24 00:13            1   \n",
       "4        1.0       1  2020-06-24 00:13            1   \n",
       "\n",
       "                                    tweets_url_token  \\\n",
       "0  @Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...   \n",
       "1  @w_terrence @PearlCathey8 Like deplorable, lik...   \n",
       "2       @HarleyQuinnlif3 Dick always ruin things lol   \n",
       "3  After seeing pics of todays rally in Phoenix I...   \n",
       "4  if I do get covid19 becuase I'm surrounded by ...   \n",
       "\n",
       "                                           cleanData  \n",
       "0     it's about time. Lol IT'S NOT FAIR [URL] [URL]  \n",
       "1  Like deplorable, like donnie, have to use cont...  \n",
       "2                        Dick always ruin things lol  \n",
       "3  After seeing pics of todays rally in Phoenix I...  \n",
       "4  if I do get covid19 becuase I'm surrounded by ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountname             0\n",
      "description           489\n",
      "favorite_count         35\n",
      "location            10421\n",
      "masked_user_id          0\n",
      "retweet_count           4\n",
      "timestamp               0\n",
      "tweets                  0\n",
      "twittername             0\n",
      "user_type               0\n",
      "format                  0\n",
      "new_date                0\n",
      "time_window             0\n",
      "tweets_url_token        0\n",
      "cleanData             244\n",
      "dtype: int64\n",
      "accountname         0\n",
      "description         0\n",
      "favorite_count      0\n",
      "location            0\n",
      "masked_user_id      0\n",
      "retweet_count       0\n",
      "timestamp           0\n",
      "tweets              0\n",
      "twittername         0\n",
      "user_type           0\n",
      "format              0\n",
      "new_date            0\n",
      "time_window         0\n",
      "tweets_url_token    0\n",
      "cleanData           0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(depressed_dataset.isnull().sum())\n",
    "\n",
    "\n",
    "depressed_dataset = depressed_dataset.replace(np.nan, '', regex=True)\n",
    "\n",
    "print(depressed_dataset.isnull().sum())\n",
    "print(\"\")\n",
    "# print(depressed_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = depressed_dataset.loc[:,'cleanData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           it's about time. Lol IT'S NOT FAIR [URL] [URL]\n",
       "1        Like deplorable, like donnie, have to use cont...\n",
       "2                              Dick always ruin things lol\n",
       "3        After seeing pics of todays rally in Phoenix I...\n",
       "4        if I do get covid19 becuase I'm surrounded by ...\n",
       "                               ...                        \n",
       "61967    You need not to be afraid anymore. You are lov...\n",
       "61968    I did register but I still didn't get the link...\n",
       "61969               You have a beautiful name too! ð???â?�\n",
       "61970                                                 yes!\n",
       "61971                                 Can I get the link??\n",
       "Name: cleanData, Length: 61972, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(String): #default value is always true for stemming and stopwords\n",
    "#     print(String)\n",
    "#     print(\"\")\n",
    "    '''\n",
    "    This function is used for preprocessing\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Stop Words\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    influential_words = re.sub(r'[^a-zA-Z0-9_\\[\\]]', ' ', String)\n",
    "#     print(influential_words)\n",
    "#     tokens = nltk.word_tokenize(String)\n",
    "#     print(tokens)\n",
    "#     token = [word for word in tokens if word.isalpha()]\n",
    "#     print(token)\n",
    "#     influential_words = \" \".join(token)\n",
    "#     print(influential_words)\n",
    "    influential_words = influential_words.lower()\n",
    "#     print(influential_words)\n",
    "#     influential_words = influential_words.split()\n",
    "#     print(influential_words)\n",
    "#     stemwords_string = \" \".join(influential_words)\n",
    "#     print(stemwords_string)\n",
    "    return influential_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = []\n",
    "for i in range(0,len(depressed_dataset.axes[0])):\n",
    "    final_text.append(str(preprocessing(tweet_text[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset['cleanData'] = final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_text,tweet_text,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean special characters from cleanData row to apply BERT embeddings\n",
    "def preprocess(tweets):\n",
    "    tweets = tweets.replace(\"(<br/>)\", \"\")\n",
    "    tweets = tweets.replace('(<a).*(>).*(</a>)', '')\n",
    "    tweets = tweets.replace('(&amp)', '')\n",
    "    tweets = tweets.replace('(&gt)', '')\n",
    "    tweets = tweets.replace('(&lt)', '')\n",
    "    tweets = tweets.replace('(â€™)', '')\n",
    "    tweets = tweets.replace('(\\xa0)', ' ')\n",
    "    tweets = tweets.replace('(https)', ' ')\n",
    "    tweets = tweets.replace('(RT)', ' ')\n",
    "    tweets = tweets.replace('(ðÿ)', '  ')\n",
    "    tweets = tweets.replace('(\\n)', '  ')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset['cleanData'] = depressed_dataset['cleanData'].apply(preprocess).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depressed_dataset_1\n",
      "(700, 15)\n",
      "\n",
      "depressed_dataset_2\n",
      "(1093, 15)\n",
      "\n",
      "depressed_dataset_3\n",
      "(3223, 15)\n",
      "\n",
      "depressed_dataset_4\n",
      "(4868, 15)\n",
      "\n",
      "depressed_dataset_5\n",
      "(5463, 15)\n",
      "\n",
      "depressed_dataset_6\n",
      "(6878, 15)\n",
      "\n",
      "depressed_dataset_7\n",
      "(7786, 15)\n",
      "\n",
      "depressed_dataset_8\n",
      "(8278, 15)\n",
      "\n",
      "depressed_dataset_9\n",
      "(8394, 15)\n",
      "\n",
      "depressed_dataset_10\n",
      "(5696, 15)\n",
      "\n",
      "depressed_dataset_11\n",
      "(4408, 15)\n",
      "\n",
      "depressed_dataset_12\n",
      "(2729, 15)\n",
      "\n",
      "depressed_dataset_13\n",
      "(1569, 15)\n",
      "\n",
      "depressed_dataset_14\n",
      "(887, 15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "depressed_dataset_1 = depressed_dataset[depressed_dataset.time_window == 1]\n",
    "depressed_dataset_2 = depressed_dataset[depressed_dataset.time_window == 2]\n",
    "depressed_dataset_3 = depressed_dataset[depressed_dataset.time_window == 3]\n",
    "depressed_dataset_4 = depressed_dataset[depressed_dataset.time_window == 4]\n",
    "depressed_dataset_5 = depressed_dataset[depressed_dataset.time_window == 5]\n",
    "depressed_dataset_6 = depressed_dataset[depressed_dataset.time_window == 6]\n",
    "depressed_dataset_7 = depressed_dataset[depressed_dataset.time_window == 7]\n",
    "depressed_dataset_8 = depressed_dataset[depressed_dataset.time_window == 8]\n",
    "depressed_dataset_9 = depressed_dataset[depressed_dataset.time_window == 9]\n",
    "depressed_dataset_10 = depressed_dataset[depressed_dataset.time_window == 10]\n",
    "depressed_dataset_11 = depressed_dataset[depressed_dataset.time_window == 11]\n",
    "depressed_dataset_12 = depressed_dataset[depressed_dataset.time_window == 12]\n",
    "depressed_dataset_13 = depressed_dataset[depressed_dataset.time_window == 13]\n",
    "depressed_dataset_14 = depressed_dataset[depressed_dataset.time_window == 14]\n",
    "\n",
    "\n",
    "for i in range(1,15):\n",
    "    filename = \"depressed_dataset_\"+str(i)\n",
    "    print(filename)\n",
    "    print(getattr(sys.modules[__name__], f\"depressed_dataset_{i}\").shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"D:\\Workspace\\OVGU\\SM_Depression\\scripts\\saved_models\\kaggle_finetune_epoch10_es_new\" # change this to your preferred location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at D:\\Workspace\\OVGU\\SM_Depression\\scripts\\saved_models\\kaggle_finetune_epoch10_es_new were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at D:\\Workspace\\OVGU\\SM_Depression\\scripts\\saved_models\\kaggle_finetune_epoch10_es_new and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = DistilBertTokenizer.from_pretrained(model_directory)\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(model_directory,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3845      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,957,317\n",
      "Trainable params: 66,957,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model._layers.pop()\n",
    "# loaded_model._layers.pop()\n",
    "# loaded_model._layers.pop()\n",
    "# loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tf.config.list_physical_devices('GPU'):\n",
    "#     physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "#     tf.config.experimental.set_virtual_device_configuration(physical_devices[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "\n",
    "# tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
    "# tf.config.gpu.set_per_process_memory_growth(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_input = loaded_tokenizer.encode(test_val,\n",
    "#                                  truncation=True,\n",
    "#                                  padding=True,\n",
    "#                                  return_tensors=\"tf\")\n",
    "\n",
    "# output = loaded_model(predict_input)[0]\n",
    "\n",
    "# prediction_value = tf.argmax(output, axis=1).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(depressed_dataset_1)\n",
    "# depressed_dataset_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "embed_path = r\"D:\\Workspace\\OVGU\\SM_Depression\\scripts\\embeddings_sentences\"\n",
    "os.chdir(embed_path)\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def get_store_embeddings(dataframe,dataframe_name):\n",
    "#     sentences = []\n",
    "#     row_count = dataframe.shape[0]\n",
    "#     for i in range(0, row_count):\n",
    "#         sentences.append(dataframe.iloc[i, 14])\n",
    "    print(dataframe_name)\n",
    "    BATCH_SIZE = 50\n",
    "    print(len(dataframe.cleanData.tolist()))\n",
    "#     embeddings = [loaded_tokenizer.encode(sent) for sent in sentences]\n",
    "    tokenized_text = loaded_tokenizer.batch_encode_plus(dataframe.cleanData.tolist(),\n",
    "                                           add_special_tokens = True,\n",
    "                                           pad_to_max_length = True,\n",
    "                                           max_length = 350,\n",
    "                                            return_token_type_ids=True,\n",
    "                                           return_tensors = 'tf')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids']).batch(BATCH_SIZE)\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((tokenized_text['input_ids'],\n",
    "#                                               tokenized_text['attention_mask'],\n",
    "#                                               tokenized_text['token_type_ids'])).batch(3)\n",
    "\n",
    "#     sample = next(iter(dataset))\n",
    "\n",
    "    embeddings_array = np.empty([BATCH_SIZE,768])\n",
    "    for batch in dataset:\n",
    "        result = loaded_model(batch)  \n",
    "\n",
    "#         print(type(result['hidden_states'][6]))\n",
    "#         print(len(result['hidden_states'][6]))\n",
    "        embeddings_batch = np.stack(list(result['hidden_states'][6][:,0,:]))\n",
    "#         print(type(embeddings_batch))\n",
    "#         print(embeddings_batch.shape)\n",
    "        embeddings_array = np.concatenate((embeddings_array, embeddings_batch), axis = 0)\n",
    "\n",
    "\n",
    "#     print(embeddings_array.shape)\n",
    "#     print(embeddings_array)\n",
    "    embeddings_array = np.delete(embeddings_array, np.s_[:BATCH_SIZE], 0)\n",
    "    print(embeddings_array.shape)\n",
    "#     print(embeddings_array)\n",
    "#     embeddings_array = embeddings_array[:,0,:]\n",
    "#     print(embeddings_array.shape)\n",
    "    embed_name = dataframe_name +\"_embedding.pkl\"\n",
    "    print(embed_name)\n",
    "#     for sentence, embedding in zip(dataframe.cleanData.tolist(), embeddings_array):\n",
    "#         print(\"Sentence:\", sentence)\n",
    "#         print(\"Embedding:\", embedding)\n",
    "#         print(embedding.shape)\n",
    "#         print(\"\")\n",
    "    #Store sentences & embeddings on disc\n",
    "    with open(embed_name, \"wb\") as fOut:\n",
    "        pickle.dump(dict(zip(dataframe.cleanData.tolist(), embeddings_array)), fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     return embeddings_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depressed_dataset_14\n",
      "887\n",
      "(887, 768)\n",
      "depressed_dataset_14_embedding.pkl\n"
     ]
    }
   ],
   "source": [
    "# get_store_embeddings(depressed_dataset_1,\"depressed_dataset_1\")\n",
    "# get_store_embeddings(depressed_dataset_2,\"depressed_dataset_2\")\n",
    "# get_store_embeddings(depressed_dataset_3,\"depressed_dataset_3\")\n",
    "# get_store_embeddings(depressed_dataset_4,\"depressed_dataset_4\")\n",
    "# get_store_embeddings(depressed_dataset_5,\"depressed_dataset_5\")\n",
    "# get_store_embeddings(depressed_dataset_6,\"depressed_dataset_6\")\n",
    "# get_store_embeddings(depressed_dataset_7,\"depressed_dataset_7\")\n",
    "# get_store_embeddings(depressed_dataset_8,\"depressed_dataset_8\")\n",
    "# get_store_embeddings(depressed_dataset_9,\"depressed_dataset_9\")\n",
    "# get_store_embeddings(depressed_dataset_10,\"depressed_dataset_10\")\n",
    "# get_store_embeddings(depressed_dataset_11,\"depressed_dataset_11\")\n",
    "# get_store_embeddings(depressed_dataset_12,\"depressed_dataset_12\")\n",
    "# get_store_embeddings(depressed_dataset_13,\"depressed_dataset_13\")\n",
    "# get_store_embeddings(depressed_dataset_14,\"depressed_dataset_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RETurning function\n",
    "\n",
    "\n",
    "# embeddings_array_1 = get_store_embeddings(depressed_dataset_1,\"depressed_dataset_1\")\n",
    "# embeddings_array_2 = get_store_embeddings(depressed_dataset_2,\"depressed_dataset_2\")\n",
    "# embeddings_array_3 = get_store_embeddings(depressed_dataset_3,\"depressed_dataset_3\")\n",
    "# embeddings_array_4 = get_store_embeddings(depressed_dataset_4,\"depressed_dataset_4\")\n",
    "# embeddings_array_5 = get_store_embeddings(depressed_dataset_5,\"depressed_dataset_5\")\n",
    "# embeddings_array_6 = get_store_embeddings(depressed_dataset_6,\"depressed_dataset_6\")\n",
    "# embeddings_array_7 = get_store_embeddings(depressed_dataset_7,\"depressed_dataset_7\")\n",
    "# embeddings_array_8 = get_store_embeddings(depressed_dataset_8,\"depressed_dataset_8\")\n",
    "# embeddings_array_9 = get_store_embeddings(depressed_dataset_9,\"depressed_dataset_9\")\n",
    "# embeddings_array_10 = get_store_embeddings(depressed_dataset_10,\"depressed_dataset_10\")\n",
    "# embeddings_array_11 = get_store_embeddings(depressed_dataset_11,\"depressed_dataset_11\")\n",
    "# embeddings_array_12 = get_store_embeddings(depressed_dataset_12,\"depressed_dataset_12\")\n",
    "# embeddings_array_13 = get_store_embeddings(depressed_dataset_13,\"depressed_dataset_13\")\n",
    "# embeddings_array_14 = get_store_embeddings(depressed_dataset_14,\"depressed_dataset_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences are encoded by calling model.encode()\n",
    "# # embeddings = [loaded_tokenizer.encode(sent,truncation=True,padding=True,add_special_tokens = True) for sent in sentences]\n",
    "# embeddings = loaded_tokenizer.batch_encode_plus(sentence,\n",
    "#                                            add_special_tokens = True,\n",
    "#                                             return_attention_masks = True,\n",
    "#                                            pad_to_max_length = True,\n",
    "#                                            max_length = 350,\n",
    "#                                            return_tensors = 'tf')['input_ids']\n",
    "\n",
    "# # print(sentence)\n",
    "# # print(embeddings)\n",
    "# #Print the embeddings\n",
    "# for sentencea, embedding in zip(sentence, embeddings):\n",
    "#     print(\"Sentence:\", sentencea)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_store_embeddings(dataframe):\n",
    "#     ### Change the dataframe.cleanData.values to your list containing data. and pass the list as arguement\n",
    "#     ###        when calling the function. If you pass dataframe, mention the column whose values you need\n",
    "#     ###        to encode like how i've mentioned here. \n",
    "#     embeddings = loaded_tokenizer.batch_encode_plus(dataframe.cleanData.values,\n",
    "#                                            add_special_tokens = True,\n",
    "#                                            pad_to_max_length = True,\n",
    "#                                            max_length = 350,\n",
    "#                                            return_tensors = 'tf')['input_ids']\n",
    "\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "# get_store_embeddings(depressed_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
