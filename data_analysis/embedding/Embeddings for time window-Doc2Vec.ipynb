{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models.doc2vec import Doc2Vec , TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset = pd.read_csv(\"D:\\Workspace\\OVGU\\SM_Depression\\src\\depTweetsOrderedByDateTimeWindowURLTokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61972 entries, 0 to 61971\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   accountname       61972 non-null  object \n",
      " 1   description       61483 non-null  object \n",
      " 2   favorite_count    61937 non-null  float64\n",
      " 3   location          51551 non-null  object \n",
      " 4   masked_user_id    61972 non-null  object \n",
      " 5   retweet_count     61968 non-null  object \n",
      " 6   timestamp         61972 non-null  object \n",
      " 7   tweets            61972 non-null  object \n",
      " 8   twittername       61972 non-null  object \n",
      " 9   user_type         61972 non-null  float64\n",
      " 10  format            61972 non-null  int64  \n",
      " 11  new_date          61972 non-null  object \n",
      " 12  time_window       61972 non-null  int64  \n",
      " 13  tweets_url_token  61972 non-null  object \n",
      " 14  cleanData         61728 non-null  object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "depressed_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accountname</th>\n",
       "      <th>description</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>location</th>\n",
       "      <th>masked_user_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweets</th>\n",
       "      <th>twittername</th>\n",
       "      <th>user_type</th>\n",
       "      <th>format</th>\n",
       "      <th>new_date</th>\n",
       "      <th>time_window</th>\n",
       "      <th>tweets_url_token</th>\n",
       "      <th>cleanData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:00</td>\n",
       "      <td>@Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>@Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...</td>\n",
       "      <td>it's about time. Lol IT'S NOT FAIR [URL] [URL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:02</td>\n",
       "      <td>@w_terrence @PearlCathey8 Like deplorable, lik...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:02</td>\n",
       "      <td>1</td>\n",
       "      <td>@w_terrence @PearlCathey8 Like deplorable, lik...</td>\n",
       "      <td>Like deplorable, like donnie, have to use cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:05</td>\n",
       "      <td>@HarleyQuinnlif3 Dick always ruin things lol</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:05</td>\n",
       "      <td>1</td>\n",
       "      <td>@HarleyQuinnlif3 Dick always ruin things lol</td>\n",
       "      <td>Dick always ruin things lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:13</td>\n",
       "      <td>After seeing pics of todays rally in Phoenix I...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>After seeing pics of todays rally in Phoenix I...</td>\n",
       "      <td>After seeing pics of todays rally in Phoenix I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cmdr. Nasty Tinkerbell of the 4077th</td>\n",
       "      <td>NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Exploring the universe</td>\n",
       "      <td>depressed_user_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24-06-2020 00:13</td>\n",
       "      <td>if I do get covid19 becuase I'm surrounded by ...</td>\n",
       "      <td>CmdrTinkerBell</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>if I do get covid19 becuase I'm surrounded by ...</td>\n",
       "      <td>if I do get covid19 becuase I'm surrounded by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accountname  \\\n",
       "0  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "1  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "2  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "3  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "4  Cmdr. Nasty Tinkerbell of the 4077th   \n",
       "\n",
       "                                         description  favorite_count  \\\n",
       "0  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             0.0   \n",
       "1  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             0.0   \n",
       "2  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             0.0   \n",
       "3  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             2.0   \n",
       "4  NSFW \\r\\r\\r\\r\\nWoman \\r\\r\\r\\r\\nDisabled \\r\\r\\r...             2.0   \n",
       "\n",
       "                  location     masked_user_id retweet_count         timestamp  \\\n",
       "0  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:00   \n",
       "1  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:02   \n",
       "2  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:05   \n",
       "3  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:13   \n",
       "4  Exploring the universe   depressed_user_11           0.0  24-06-2020 00:13   \n",
       "\n",
       "                                              tweets     twittername  \\\n",
       "0  @Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...  CmdrTinkerBell   \n",
       "1  @w_terrence @PearlCathey8 Like deplorable, lik...  CmdrTinkerBell   \n",
       "2       @HarleyQuinnlif3 Dick always ruin things lol  CmdrTinkerBell   \n",
       "3  After seeing pics of todays rally in Phoenix I...  CmdrTinkerBell   \n",
       "4  if I do get covid19 becuase I'm surrounded by ...  CmdrTinkerBell   \n",
       "\n",
       "   user_type  format          new_date  time_window  \\\n",
       "0        1.0       1  2020-06-24 00:00            1   \n",
       "1        1.0       1  2020-06-24 00:02            1   \n",
       "2        1.0       1  2020-06-24 00:05            1   \n",
       "3        1.0       1  2020-06-24 00:13            1   \n",
       "4        1.0       1  2020-06-24 00:13            1   \n",
       "\n",
       "                                    tweets_url_token  \\\n",
       "0  @Twitter it's about time.\\r\\r\\r\\r\\n\\r\\r\\r\\r\\nL...   \n",
       "1  @w_terrence @PearlCathey8 Like deplorable, lik...   \n",
       "2       @HarleyQuinnlif3 Dick always ruin things lol   \n",
       "3  After seeing pics of todays rally in Phoenix I...   \n",
       "4  if I do get covid19 becuase I'm surrounded by ...   \n",
       "\n",
       "                                           cleanData  \n",
       "0     it's about time. Lol IT'S NOT FAIR [URL] [URL]  \n",
       "1  Like deplorable, like donnie, have to use cont...  \n",
       "2                        Dick always ruin things lol  \n",
       "3  After seeing pics of todays rally in Phoenix I...  \n",
       "4  if I do get covid19 becuase I'm surrounded by ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountname             0\n",
      "description           489\n",
      "favorite_count         35\n",
      "location            10421\n",
      "masked_user_id          0\n",
      "retweet_count           4\n",
      "timestamp               0\n",
      "tweets                  0\n",
      "twittername             0\n",
      "user_type               0\n",
      "format                  0\n",
      "new_date                0\n",
      "time_window             0\n",
      "tweets_url_token        0\n",
      "cleanData             244\n",
      "dtype: int64\n",
      "accountname         0\n",
      "description         0\n",
      "favorite_count      0\n",
      "location            0\n",
      "masked_user_id      0\n",
      "retweet_count       0\n",
      "timestamp           0\n",
      "tweets              0\n",
      "twittername         0\n",
      "user_type           0\n",
      "format              0\n",
      "new_date            0\n",
      "time_window         0\n",
      "tweets_url_token    0\n",
      "cleanData           0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(depressed_dataset.isnull().sum())\n",
    "\n",
    "\n",
    "depressed_dataset = depressed_dataset.replace(np.nan, '', regex=True)\n",
    "\n",
    "print(depressed_dataset.isnull().sum())\n",
    "print(\"\")\n",
    "# print(depressed_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = depressed_dataset.loc[:,'cleanData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           it's about time. Lol IT'S NOT FAIR [URL] [URL]\n",
       "1        Like deplorable, like donnie, have to use cont...\n",
       "2                              Dick always ruin things lol\n",
       "3        After seeing pics of todays rally in Phoenix I...\n",
       "4        if I do get covid19 becuase I'm surrounded by ...\n",
       "                               ...                        \n",
       "61967    You need not to be afraid anymore. You are lov...\n",
       "61968    I did register but I still didn't get the link...\n",
       "61969               You have a beautiful name too! ð???â?�\n",
       "61970                                                 yes!\n",
       "61971                                 Can I get the link??\n",
       "Name: cleanData, Length: 61972, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(String): #default value is always true for stemming and stopwords\n",
    "#     print(String)\n",
    "#     print(\"\")\n",
    "    '''\n",
    "    This function is used for preprocessing\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Stop Words\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    influential_words = re.sub(r'[^a-zA-Z0-9_\\[\\]]', ' ', String)\n",
    "#     print(influential_words)\n",
    "#     tokens = nltk.word_tokenize(String)\n",
    "#     print(tokens)\n",
    "#     token = [word for word in tokens if word.isalpha()]\n",
    "#     print(token)\n",
    "#     influential_words = \" \".join(token)\n",
    "#     print(influential_words)\n",
    "    influential_words = influential_words.lower()\n",
    "#     print(influential_words)\n",
    "#     influential_words = influential_words.split()\n",
    "#     print(influential_words)\n",
    "#     stemwords_string = \" \".join(influential_words)\n",
    "#     print(stemwords_string)\n",
    "    return influential_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = []\n",
    "for i in range(0,len(depressed_dataset.axes[0])):\n",
    "    final_text.append(str(preprocessing(tweet_text[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset['cleanData'] = final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_text,tweet_text,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean special characters from cleanData row to apply BERT embeddings\n",
    "def preprocess(tweets):\n",
    "    tweets = tweets.replace(\"(<br/>)\", \"\")\n",
    "    tweets = tweets.replace('(<a).*(>).*(</a>)', '')\n",
    "    tweets = tweets.replace('(&amp)', '')\n",
    "    tweets = tweets.replace('(&gt)', '')\n",
    "    tweets = tweets.replace('(&lt)', '')\n",
    "    tweets = tweets.replace('(â€™)', '')\n",
    "    tweets = tweets.replace('(\\xa0)', ' ')\n",
    "    tweets = tweets.replace('(https)', ' ')\n",
    "    tweets = tweets.replace('(RT)', ' ')\n",
    "    tweets = tweets.replace('(ðÿ)', '  ')\n",
    "    tweets = tweets.replace('(\\n)', '  ')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset['cleanData'] = depressed_dataset['cleanData'].apply(preprocess).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depressed_dataset_1\n",
      "(700, 15)\n",
      "\n",
      "depressed_dataset_2\n",
      "(1093, 15)\n",
      "\n",
      "depressed_dataset_3\n",
      "(3223, 15)\n",
      "\n",
      "depressed_dataset_4\n",
      "(4868, 15)\n",
      "\n",
      "depressed_dataset_5\n",
      "(5463, 15)\n",
      "\n",
      "depressed_dataset_6\n",
      "(6878, 15)\n",
      "\n",
      "depressed_dataset_7\n",
      "(7786, 15)\n",
      "\n",
      "depressed_dataset_8\n",
      "(8278, 15)\n",
      "\n",
      "depressed_dataset_9\n",
      "(8394, 15)\n",
      "\n",
      "depressed_dataset_10\n",
      "(5696, 15)\n",
      "\n",
      "depressed_dataset_11\n",
      "(4408, 15)\n",
      "\n",
      "depressed_dataset_12\n",
      "(2729, 15)\n",
      "\n",
      "depressed_dataset_13\n",
      "(1569, 15)\n",
      "\n",
      "depressed_dataset_14\n",
      "(887, 15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "depressed_dataset_1 = depressed_dataset[depressed_dataset.time_window == 1]\n",
    "depressed_dataset_2 = depressed_dataset[depressed_dataset.time_window == 2]\n",
    "depressed_dataset_3 = depressed_dataset[depressed_dataset.time_window == 3]\n",
    "depressed_dataset_4 = depressed_dataset[depressed_dataset.time_window == 4]\n",
    "depressed_dataset_5 = depressed_dataset[depressed_dataset.time_window == 5]\n",
    "depressed_dataset_6 = depressed_dataset[depressed_dataset.time_window == 6]\n",
    "depressed_dataset_7 = depressed_dataset[depressed_dataset.time_window == 7]\n",
    "depressed_dataset_8 = depressed_dataset[depressed_dataset.time_window == 8]\n",
    "depressed_dataset_9 = depressed_dataset[depressed_dataset.time_window == 9]\n",
    "depressed_dataset_10 = depressed_dataset[depressed_dataset.time_window == 10]\n",
    "depressed_dataset_11 = depressed_dataset[depressed_dataset.time_window == 11]\n",
    "depressed_dataset_12 = depressed_dataset[depressed_dataset.time_window == 12]\n",
    "depressed_dataset_13 = depressed_dataset[depressed_dataset.time_window == 13]\n",
    "depressed_dataset_14 = depressed_dataset[depressed_dataset.time_window == 14]\n",
    "\n",
    "\n",
    "for i in range(1,15):\n",
    "    filename = \"depressed_dataset_\"+str(i)\n",
    "    print(filename)\n",
    "    print(getattr(sys.modules[__name__], f\"depressed_dataset_{i}\").shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./saved_models/doc2vec_new_300_5_v2/kaggleTune_500_3.d2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "embed_path = r\"D:\\Workspace\\OVGU\\SM_Depression\\scripts\\doc2vec_embeddings\"\n",
    "os.chdir(embed_path)\n",
    "\n",
    "def get_store_embeddings(dataframe,dataframe_name):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(dataframe['cleanData'])]\n",
    "    \n",
    "    inferred_vectors = [model.infer_vector(doc.words, steps=20) for doc in tagged_data]\n",
    "    \n",
    "    print(len(dataframe['cleanData']))\n",
    "    print(len(inferred_vectors))\n",
    "    print(len(tagged_data))\n",
    "    embed_name = dataframe_name +\"_embedding.pkl\"\n",
    "    print(embed_name)\n",
    "#     for sentence, embedding in zip(dataframe.cleanData.tolist(), embeddings_array):\n",
    "#         print(\"Sentence:\", sentence)\n",
    "#         print(\"Embedding:\", embedding)\n",
    "#         print(embedding.shape)\n",
    "#         print(\"\")\n",
    "    #Store sentences & embeddings on disc\n",
    "    with open(embed_name, \"wb\") as fOut:\n",
    "        pickle.dump(OrderedDict(zip(dataframe.cleanData.tolist(), inferred_vectors)), fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "700\n",
      "700\n",
      "depressed_dataset_1_embedding.pkl\n",
      "1093\n",
      "1093\n",
      "1093\n",
      "depressed_dataset_2_embedding.pkl\n",
      "3223\n",
      "3223\n",
      "3223\n",
      "depressed_dataset_3_embedding.pkl\n",
      "4868\n",
      "4868\n",
      "4868\n",
      "depressed_dataset_4_embedding.pkl\n",
      "5463\n",
      "5463\n",
      "5463\n",
      "depressed_dataset_5_embedding.pkl\n",
      "6878\n",
      "6878\n",
      "6878\n",
      "depressed_dataset_6_embedding.pkl\n",
      "7786\n",
      "7786\n",
      "7786\n",
      "depressed_dataset_7_embedding.pkl\n",
      "8278\n",
      "8278\n",
      "8278\n",
      "depressed_dataset_8_embedding.pkl\n",
      "8394\n",
      "8394\n",
      "8394\n",
      "depressed_dataset_9_embedding.pkl\n",
      "5696\n",
      "5696\n",
      "5696\n",
      "depressed_dataset_10_embedding.pkl\n",
      "4408\n",
      "4408\n",
      "4408\n",
      "depressed_dataset_11_embedding.pkl\n",
      "2729\n",
      "2729\n",
      "2729\n",
      "depressed_dataset_12_embedding.pkl\n",
      "1569\n",
      "1569\n",
      "1569\n",
      "depressed_dataset_13_embedding.pkl\n",
      "887\n",
      "887\n",
      "887\n",
      "depressed_dataset_14_embedding.pkl\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "get_store_embeddings(depressed_dataset_1,\"depressed_dataset_1\")\n",
    "get_store_embeddings(depressed_dataset_2,\"depressed_dataset_2\")\n",
    "get_store_embeddings(depressed_dataset_3,\"depressed_dataset_3\")\n",
    "get_store_embeddings(depressed_dataset_4,\"depressed_dataset_4\")\n",
    "get_store_embeddings(depressed_dataset_5,\"depressed_dataset_5\")\n",
    "get_store_embeddings(depressed_dataset_6,\"depressed_dataset_6\")\n",
    "get_store_embeddings(depressed_dataset_7,\"depressed_dataset_7\")\n",
    "get_store_embeddings(depressed_dataset_8,\"depressed_dataset_8\")\n",
    "get_store_embeddings(depressed_dataset_9,\"depressed_dataset_9\")\n",
    "get_store_embeddings(depressed_dataset_10,\"depressed_dataset_10\")\n",
    "get_store_embeddings(depressed_dataset_11,\"depressed_dataset_11\")\n",
    "get_store_embeddings(depressed_dataset_12,\"depressed_dataset_12\")\n",
    "get_store_embeddings(depressed_dataset_13,\"depressed_dataset_13\")\n",
    "get_store_embeddings(depressed_dataset_14,\"depressed_dataset_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
