{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_1_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    477\n",
       "1    207\n",
       "2     16\n",
       "Name: cluster_label, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.cluster_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.032932799287939477), ('like', 0.012164367304554221), ('amp', 0.010977599762646492), ('people', 0.008455718736092568), ('u', 0.008307372793354102), ('get', 0.006972259308707907), ('one', 0.006378875537754043), ('know', 0.006230529595015576), ('black', 0.005637145824061712), ('love', 0.0053404539385847796), ('going', 0.005192107995846314), ('think', 0.004302032339415517), ('lol', 0.004005340453938585), ('need', 0.0037086485684616525), ('never', 0.0037086485684616525), ('time', 0.0035603026257231864), ('things', 0.0035603026257231864), ('still', 0.0035603026257231864), ('back', 0.0034119566829847203), ('getting', 0.003263610740246254), ('shit', 0.003263610740246254), ('see', 0.003263610740246254), ('white', 0.003263610740246254), ('got', 0.003115264797507788), ('techno', 0.003115264797507788), ('okay', 0.002966918854769322), ('really', 0.002966918854769322), ('want', 0.002966918854769322), ('year', 0.002818572912030856), ('life', 0.002818572912030856)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_2_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1093"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.0395897925113284), ('like', 0.012401621750536608), ('amp', 0.009301216312902457), ('love', 0.0076317672310994514), ('one', 0.006200810875268304), ('lol', 0.006081564512282376), ('get', 0.00560457906033866), ('u', 0.005246839971380873), ('people', 0.004650608156451228), ('know', 0.004531361793465299), ('go', 0.004292869067493442), ('really', 0.004292869067493442), ('want', 0.004292869067493442), ('think', 0.004054376341521584), ('got', 0.004054376341521584), ('time', 0.0036966372525637967), ('gonna', 0.0036966372525637967), ('good', 0.0035773908895778677), ('still', 0.0035773908895778677), ('never', 0.00333889816360601), ('much', 0.00333889816360601), ('thank', 0.003219651800620081), ('always', 0.003100405437634152), ('need', 0.003100405437634152), ('even', 0.003100405437634152), ('2', 0.003100405437634152), ('would', 0.002981159074648223), ('also', 0.0028619127116622945), ('shit', 0.0026234199856904365), ('3', 0.0026234199856904365)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_3_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3223"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.03713547995139733), ('like', 0.011770959902794654), ('amp', 0.00744228432563791), ('get', 0.0062272174969623326), ('one', 0.005771567436208992), ('people', 0.005239975698663427), ('know', 0.004784325637910085), ('love', 0.004594471445929526), ('time', 0.004518529769137303), ('would', 0.0041767922235722964), ('think', 0.004062879708383961), ('go', 0.0038730255164034022), ('lol', 0.0038730255164034022), ('got', 0.0038730255164034022), ('even', 0.0037970838396111785), ('want', 0.0037970838396111785), ('really', 0.003531287970838396), ('good', 0.0034553462940461726), ('thank', 0.003341433778857837), ('much', 0.003265492102065614), ('still', 0.003265492102065614), ('shit', 0.0031136087484811663), ('back', 0.003037667071688943), ('feel', 0.003037667071688943), ('need', 0.002885783718104496), ('never', 0.002885783718104496), ('right', 0.002847812879708384), ('day', 0.002809842041312272), ('also', 0.0027718712029161604), ('make', 0.0027718712029161604)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_4_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4868"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.039344922140323514), ('like', 0.012276420718975624), ('people', 0.006591029156499208), ('one', 0.006339463158159543), ('amp', 0.005911800960982113), ('know', 0.005383512364468818), ('get', 0.0051571029659631205), ('time', 0.004528187970113959), ('love', 0.004326935171442228), ('thank', 0.00402505597343463), ('would', 0.0038489597745968657), ('really', 0.003823803174762899), ('good', 0.0037986465749289327), ('much', 0.0036728635757591003), ('think', 0.0036225503760911673), ('never', 0.0033961409775854694), ('lol', 0.0033709843777515033), ('go', 0.003119418379411839), ('men', 0.0029936353802420064), ('got', 0.0029936353802420064), ('even', 0.00296847878040804), ('see', 0.00296847878040804), ('day', 0.0029433221805740734), ('need', 0.002842695781238208), ('way', 0.0027672259817363085), ('still', 0.0027672259817363085), ('shit', 0.0027672259817363085), ('back', 0.002742069381902342), ('right', 0.0027169127820683755), ('want', 0.002691756182234409)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_5_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5463"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.03827583793996221), ('like', 0.011685676299769085), ('amp', 0.006437618081309915), ('one', 0.005784526391901663), ('people', 0.005714552282322208), ('get', 0.005504629953583841), ('know', 0.005038135889720803), ('love', 0.004968161780141348), ('think', 0.004291745387539943), ('time', 0.004035173652415273), ('good', 0.003918550136449514), ('would', 0.0038952254332563618), ('make', 0.0036619784013248433), ('really', 0.0035686795885522355), ('thank', 0.0034054066662001724), ('much', 0.0032887831502344134), ('see', 0.003172159634268654), ('never', 0.0031021855246891983), ('u', 0.0030555361183028946), ('got', 0.003008886711916591), ('go', 0.002962237305530287), ('back', 0.002962237305530287), ('want', 0.0028922631959508315), ('lol', 0.00286893849275768), ('even', 0.00286893849275768), ('day', 0.0028222890863713757), ('still', 0.002659016164019313), ('right', 0.002659016164019313), ('also', 0.002612366757633009), ('yes', 0.0025657173512467052)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_6_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6878"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.029683768492250486), ('like', 0.010770784247728038), ('people', 0.0074209421230626215), ('amp', 0.006491320863586094), ('get', 0.005689923226106329), ('one', 0.0055136157458607815), ('know', 0.004872497635876969), ('think', 0.004535910628135468), ('time', 0.004471798817137087), ('love', 0.0043756311006395154), ('would', 0.004135211809395586), ('really', 0.0035261496049109648), ('good', 0.003478065746662179), ('even', 0.003365870077415012), ('also', 0.003221618502668654), ('want', 0.003173534644419868), ('much', 0.003109422833421487), ('go', 0.003061338975172701), ('day', 0.0028529755894279624), ('got', 0.002836947636678367), ('still', 0.002820919683928772), ('feel', 0.0027728358256799857), ('way', 0.0026926960619320095), ('never', 0.002676668109182414), ('make', 0.002676668109182414), ('right', 0.002564472439935247), ('going', 0.002564472439935247), ('back', 0.002484332676187271), ('someone', 0.002484332676187271), ('say', 0.002484332676187271)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_7_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7786"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.02966664740886244), ('like', 0.010168120630885666), ('people', 0.008218267953087988), ('amp', 0.006268415275290312), ('get', 0.006095095037263851), ('one', 0.005185163787624935), ('know', 0.004751863192558784), ('think', 0.004549656248194581), ('love', 0.004333005950661505), ('time', 0.004260789184817147), ('even', 0.0037408284707377663), ('would', 0.0037263851175688948), ('good', 0.003581951585880178), ('really', 0.0034952914668669476), ('much', 0.0033219712288404877), ('never', 0.0032353111098272574), ('go', 0.003134207637645156), ('also', 0.003134207637645156), ('want', 0.003076434224969669), ('need', 0.003076434224969669), ('see', 0.003033104165463054), ('feel', 0.0028597839274365935), ('day', 0.0028308972210988504), ('back', 0.0028164538679299788), ('make', 0.0028020105147611067), ('way', 0.002787567161592235), ('still', 0.002787567161592235), ('thank', 0.0025998035703969032), ('right', 0.0025131434513836733), ('u', 0.0024842567450459297)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_8_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8278"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.02681174021089395), ('like', 0.011122305057424944), ('people', 0.00694472429310229), ('get', 0.006810396937336288), ('one', 0.005453690644099671), ('think', 0.004916381221035664), ('know', 0.004741755658539862), ('amp', 0.004311908120088656), ('good', 0.004096984350863054), ('time', 0.004083551615286453), ('even', 0.004070118879709853), ('love', 0.0038283296393310496), ('would', 0.0034925112499160456), ('really', 0.0034925112499160456), ('much', 0.003304452951843643), ('go', 0.003237289273960642), ('need', 0.0031835583316542414), ('also', 0.003129827389347841), ('want', 0.00308952918261804), ('make', 0.0030492309758882396), ('right', 0.003008932769158439), ('see', 0.002995500033581839), ('got', 0.002995500033581839), ('thank', 0.002928336355698838), ('still', 0.002914903620122238), ('feel', 0.002834307206662637), ('never', 0.002699979850896635), ('someone', 0.002552219759554033), ('going', 0.002538787023977433), ('lol', 0.002511921552824233)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_9_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8394"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.028398454614251004), ('like', 0.01124520386080907), ('people', 0.00743484552780765), ('one', 0.006200130109796737), ('get', 0.005961152932117205), ('know', 0.005323880458305121), ('amp', 0.005297327438562951), ('think', 0.004261759668618314), ('time', 0.004128994569907463), ('would', 0.004049335510680952), ('good', 0.004049335510680952), ('love', 0.0037970818231303355), ('really', 0.003425339546739953), ('even', 0.0033987865269977826), ('much', 0.003292574448029102), ('thank', 0.003146532839447166), ('right', 0.003106703309833911), ('also', 0.003093426799962826), ('still', 0.0030403207604784853), ('got', 0.0029473851913808897), ('feel', 0.002881002642025464), ('never', 0.002867726132154379), ('make', 0.002827896602541124), ('go', 0.0027880670729278686), ('see', 0.0027349610334435285), ('want', 0.0027216845235724435), ('back', 0.002681854993959188), ('need', 0.0026287489544748474), ('someone', 0.0026287489544748474), ('day', 0.002522536875506167)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_10_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5696"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.025976290097629008), ('like', 0.010382767704943436), ('people', 0.007302804896947157), ('get', 0.006314892298155897), ('one', 0.0056175422284208895), ('think', 0.005036417170308383), ('time', 0.004494033782736712), ('know', 0.004397179606384628), ('good', 0.003874167054083372), ('love', 0.003467379513404618), ('would', 0.003448008678134201), ('much', 0.0034286378428637844), ('amp', 0.0032543003254300326), ('really', 0.0032155586548891987), ('day', 0.0032155586548891987), ('want', 0.0031574461490779483), ('even', 0.0031187044785371144), ('go', 0.00302185030218503), ('also', 0.002886254455292112), ('see', 0.0028475127847512787), ('life', 0.0028281419494808617), ('things', 0.002770029443669611), ('still', 0.002770029443669611), ('got', 0.002770029443669611), ('right', 0.0027119169378583604), ('us', 0.0026925461025879434), ('going', 0.002615062761506276), ('yeah', 0.0025763210909654425), ('make', 0.0025569502556950256), ('thank', 0.0025375794204246086)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_11_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4408"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.027350203225383507), ('like', 0.010226825750622788), ('people', 0.007132555395306149), ('one', 0.006083650190114068), ('get', 0.005768978628556444), ('time', 0.004851186574013374), ('good', 0.004352956601547135), ('would', 0.004195620820768323), ('think', 0.004090730300249115), ('know', 0.004012062409859709), ('really', 0.0037498361085616886), ('amp', 0.0036711682181722826), ('love', 0.0035400550675232724), ('still', 0.003172938245706044), ('even', 0.0031467156155762423), ('much', 0.003015602464927232), ('way', 0.003015602464927232), ('see', 0.0029893798347974303), ('day', 0.0029893798347974303), ('want', 0.002936934574537826), ('need', 0.0029107119444080243), ('go', 0.002805821423888816), ('also', 0.002753376163629212), ('make', 0.002700930903369608), ('right', 0.002491149862331192), ('2', 0.002491149862331192), ('well', 0.002491149862331192), ('today', 0.002491149862331192), ('yeah', 0.0024649272322013896), ('feel', 0.0024387046020715878)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_12_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2729"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.027133610527669424), ('like', 0.012216554502979125), ('people', 0.007244202494749025), ('one', 0.00591538428565305), ('think', 0.005186677525826225), ('know', 0.00454370097303785), ('love', 0.004415105662480175), ('get', 0.0042865103519225), ('really', 0.0042007801448840495), ('would', 0.00385785931673025), ('go', 0.0037721291096918), ('good', 0.003729264006172575), ('time', 0.0036006686956149), ('make', 0.0033434780744995497), ('much', 0.003214882763941875), ('even', 0.003129152556903425), ('want', 0.0030862874533842), ('also', 0.0030434223498649747), ('someone', 0.00300055724634575), ('back', 0.0029148270393073), ('way', 0.0029148270393073), ('feel', 0.0028719619357880748), ('say', 0.00282909683226885), ('see', 0.002786231728749625), ('still', 0.0027433666252304), ('going', 0.0027433666252304), ('never', 0.0025719062111534997), ('amp', 0.00248617600411505), ('sorry', 0.002443310900595825), ('need', 0.002443310900595825)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_13_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1569"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.025343953656770456), ('like', 0.012237509051412021), ('people', 0.007892831281679943), ('get', 0.006444605358435916), ('one', 0.005792903692976104), ('know', 0.0052136133236784935), ('really', 0.004923968139029689), ('time', 0.004561911658218682), ('think', 0.004561911658218682), ('also', 0.004127443881245474), ('yeah', 0.00391020999275887), ('need', 0.00391020999275887), ('would', 0.003765387400434468), ('good', 0.0036929761042722663), ('love', 0.0036929761042722663), ('still', 0.003548153511947864), ('want', 0.0034757422157856628), ('well', 0.003113685734974656), ('go', 0.003113685734974656), ('even', 0.0029688631426502536), ('feel', 0.002896451846488052), ('got', 0.0028240405503258507), ('sorry', 0.0026792179580014484), ('right', 0.0026068066618392467), ('back', 0.0026068066618392467), ('never', 0.0025343953656770456), ('way', 0.0025343953656770456), ('yes', 0.0024619840695148444), ('make', 0.0023171614771904415), ('much', 0.0023171614771904415)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timewindow 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels = pd.read_csv(r\"D:\\Workspace\\OVGU\\SM_Depression\\src\\time_window\\kmeans_cluster_doc2vec\\depTweets_window_14_kmeans_withLabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_dataset_with_labels['cleanData'] = depressed_dataset_with_labels['cleanData'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_user_id    object\n",
       "new_date          object\n",
       "time_window        int64\n",
       "cleanData         object\n",
       "cluster_label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_dataset_with_labels.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = depressed_dataset_with_labels['cleanData'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_most_common_words(cleanData,num):\n",
    "    cleanData = \" \".join(cleanData)\n",
    "    words_split = cleanData.split()\n",
    "    \n",
    "    influentialwords = []\n",
    "    for w in words_split:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "    influwordslength = len(influentialwords)\n",
    "    fdist = FreqDist(influentialwords)\n",
    "    top_words = fdist.most_common(num)\n",
    "    \n",
    "    percentages = [(instance, count / influwordslength) for instance, count in top_words]\n",
    "    print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[url]', 0.027503337783711616), ('like', 0.010013351134846462), ('one', 0.007610146862483311), ('think', 0.006275033377837116), ('people', 0.006141522029372497), ('get', 0.005473965287049399), ('good', 0.0053404539385847796), ('even', 0.004806408544726302), ('really', 0.004405874499332443), ('someone', 0.004405874499332443), ('also', 0.004272363150867824), ('love', 0.0041388518024032045), ('know', 0.0038718291054739653), ('make', 0.0038718291054739653), ('amp', 0.003738317757009346), ('want', 0.003604806408544726), ('much', 0.003604806408544726), ('gt', 0.003604806408544726), ('would', 0.003471295060080107), ('still', 0.003471295060080107), ('time', 0.0033377837116154874), ('well', 0.0033377837116154874), ('yeah', 0.0033377837116154874), ('go', 0.0032042723631508676), ('never', 0.0032042723631508676), ('lol', 0.002937249666221629), ('see', 0.002803738317757009), ('always', 0.002803738317757009), ('light', 0.002803738317757009), ('got', 0.002803738317757009)]\n"
     ]
    }
   ],
   "source": [
    "get_most_common_words(cleanData,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
